import os
import json
import torch
import logging
import argparse
from pathlib import Path
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from peft import PeftModel

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def validate_paths(lora_path, base_model_name, output_path):
    """
    éªŒè¯è¾“å…¥è·¯å¾„å’Œå‚æ•°
    
    Args:
        lora_path: LoRAé€‚é…å™¨è·¯å¾„
        base_model_name: åŸºç¡€æ¨¡å‹åç§°
        output_path: è¾“å‡ºè·¯å¾„
    """
    # æ£€æŸ¥LoRAè·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(lora_path):
        raise FileNotFoundError(f"LoRAé€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {lora_path}")
    
    # æ£€æŸ¥LoRAè·¯å¾„ä¸­æ˜¯å¦åŒ…å«å¿…è¦æ–‡ä»¶
    required_files = ["adapter_config.json", "adapter_model.safetensors"]
    for file in required_files:
        file_path = os.path.join(lora_path, file)
        if not os.path.exists(file_path):
            logger.warning(f"LoRAæ–‡ä»¶å¯èƒ½ç¼ºå¤±: {file_path}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(output_path)
    output_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"è¾“å‡ºç›®å½•å·²å‡†å¤‡: {output_path}")

def load_label_mapping(lora_path):
    """
    åŠ è½½æ ‡ç­¾æ˜ å°„æ–‡ä»¶
    
    Args:
        lora_path: LoRAé€‚é…å™¨è·¯å¾„
        
    Returns:
        dict: æ ‡ç­¾æ˜ å°„å­—å…¸
    """
    label_mapping_path = os.path.join(lora_path, "label_mapping.json")
    
    if os.path.exists(label_mapping_path):
        try:
            with open(label_mapping_path, 'r', encoding='utf-8') as f:
                label_mapping = json.load(f)
            logger.info(f"åŠ è½½æ ‡ç­¾æ˜ å°„æˆåŠŸï¼Œå…± {len(label_mapping.get('id2label', {}))} ä¸ªç±»åˆ«")
            return label_mapping
        except Exception as e:
            logger.error(f"åŠ è½½æ ‡ç­¾æ˜ å°„å¤±è´¥: {str(e)}")
            raise
    else:
        logger.warning(f"æ ‡ç­¾æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {label_mapping_path}")
        return None

def merge_lora_model(lora_path, base_model_name, output_path):
    """
    åˆå¹¶LoRAé€‚é…å™¨ä¸åŸºç¡€æ¨¡å‹
    
    Args:
        lora_path: LoRAé€‚é…å™¨è·¯å¾„
        base_model_name: åŸºç¡€æ¨¡å‹åç§°  
        output_path: åˆå¹¶åæ¨¡å‹ä¿å­˜è·¯å¾„
    """
    try:
        logger.info("=" * 60)
        logger.info("å¼€å§‹LoRAæ¨¡å‹åˆå¹¶è¿‡ç¨‹")
        logger.info("=" * 60)
        
        # éªŒè¯è¾“å…¥å‚æ•°
        validate_paths(lora_path, base_model_name, output_path)
        
        # åŠ è½½æ ‡ç­¾æ˜ å°„
        label_mapping = load_label_mapping(lora_path)
        num_labels = len(label_mapping["id2label"]) if label_mapping else 2
        
        logger.info(f"LoRAé€‚é…å™¨è·¯å¾„: {lora_path}")
        logger.info(f"åŸºç¡€æ¨¡å‹: {base_model_name}")
        logger.info(f"è¾“å‡ºè·¯å¾„: {output_path}")
        logger.info(f"æ ‡ç­¾æ•°é‡: {num_labels}")
        
        # åŠ è½½åˆ†è¯å™¨
        logger.info("æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_name, 
            trust_remote_code=True
        )
        
        # è®¾ç½®pad_token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            logger.info("è®¾ç½®pad_tokenä¸ºeos_token")
        
        logger.info("åˆ†è¯å™¨åŠ è½½å®Œæˆ")
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        logger.info("æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹...")
        base_model = AutoModelForSequenceClassification.from_pretrained(
            base_model_name,
            num_labels=num_labels,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        logger.info("åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # åŠ è½½LoRAé€‚é…å™¨
        logger.info("æ­£åœ¨åŠ è½½LoRAé€‚é…å™¨...")
        model = PeftModel.from_pretrained(base_model, lora_path)
        logger.info("LoRAé€‚é…å™¨åŠ è½½å®Œæˆ")
        
        # åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹
        logger.info("æ­£åœ¨åˆå¹¶LoRAæƒé‡...")
        merged_model = model.merge_and_unload()
        logger.info("LoRAæƒé‡åˆå¹¶å®Œæˆ")
        
        # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
        logger.info("æ­£åœ¨ä¿å­˜åˆå¹¶åçš„æ¨¡å‹...")
        merged_model.save_pretrained(
            output_path,
            safe_serialization=True,
            max_shard_size="2GB"
        )
        logger.info("æ¨¡å‹ä¿å­˜å®Œæˆ")
        
        # ä¿å­˜åˆ†è¯å™¨
        logger.info("æ­£åœ¨ä¿å­˜åˆ†è¯å™¨...")
        tokenizer.save_pretrained(output_path)
        logger.info("åˆ†è¯å™¨ä¿å­˜å®Œæˆ")
        
        # ä¿å­˜æ ‡ç­¾æ˜ å°„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if label_mapping:
            label_mapping_output_path = os.path.join(output_path, "label_mapping.json")
            with open(label_mapping_output_path, 'w', encoding='utf-8') as f:
                json.dump(label_mapping, f, ensure_ascii=False, indent=2)
            logger.info("æ ‡ç­¾æ˜ å°„ä¿å­˜å®Œæˆ")
        
        # ä¿å­˜æ¨¡å‹é…ç½®ä¿¡æ¯
        config_info = {
            "base_model": base_model_name,
            "lora_adapter": lora_path,
            "num_labels": num_labels,
            "merge_timestamp": str(torch.cuda.current_device() if torch.cuda.is_available() else "cpu"),
            "model_type": "merged_lora_model"
        }
        
        config_path = os.path.join(output_path, "merge_info.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config_info, f, ensure_ascii=False, indent=2)
        logger.info("åˆå¹¶ä¿¡æ¯ä¿å­˜å®Œæˆ")
        
        logger.info("=" * 60)
        logger.info("LoRAæ¨¡å‹åˆå¹¶æˆåŠŸå®Œæˆï¼")
        logger.info(f"åˆå¹¶åçš„æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
        logger.info("=" * 60)
        
        # æ˜¾ç¤ºè¾“å‡ºç›®å½•å†…å®¹
        logger.info("è¾“å‡ºç›®å½•å†…å®¹:")
        for item in os.listdir(output_path):
            item_path = os.path.join(output_path, item)
            if os.path.isfile(item_path):
                size = os.path.getsize(item_path) / (1024 * 1024)  # MB
                logger.info(f"  - {item} ({size:.2f} MB)")
            else:
                logger.info(f"  - {item}/ (ç›®å½•)")
        
    except Exception as e:
        logger.error(f"æ¨¡å‹åˆå¹¶è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        raise

def main():
    """
    ä¸»å‡½æ•°ï¼Œå¤„ç†å‘½ä»¤è¡Œå‚æ•°å¹¶æ‰§è¡Œæ¨¡å‹åˆå¹¶
    """
    parser = argparse.ArgumentParser(
        description="åˆå¹¶LoRAé€‚é…å™¨ä¸Qwen3åŸºç¡€æ¨¡å‹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python merge_model.py
  python merge_model.py --lora_path ./custom-lora --output_path ./custom-merged
  python merge_model.py --base_model Qwen/Qwen3-7B --lora_path ./lora --output_path ./merged
        """
    )
    
    parser.add_argument(
        "--lora_path",
        type=str,
        default="./qwen3-intent-lora",
        help="LoRAé€‚é…å™¨è·¯å¾„ (é»˜è®¤: ./qwen3-intent-lora)"
    )
    
    parser.add_argument(
        "--base_model",
        type=str,
        default="Qwen/Qwen3-8B",
        help="åŸºç¡€æ¨¡å‹åç§° (é»˜è®¤: Qwen/Qwen3-8B)"
    )
    
    parser.add_argument(
        "--output_path",
        type=str,
        default="./qwen3-intent-merged",
        help="åˆå¹¶åæ¨¡å‹ä¿å­˜è·¯å¾„ (é»˜è®¤: ./qwen3-intent-merged)"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—ä¿¡æ¯"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        # æ‰§è¡Œæ¨¡å‹åˆå¹¶
        merge_lora_model(
            lora_path=args.lora_path,
            base_model_name=args.base_model,
            output_path=args.output_path
        )
        
        print("\nâœ… æ¨¡å‹åˆå¹¶æˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“ åˆå¹¶åçš„æ¨¡å‹ä¿å­˜åœ¨: {args.output_path}")
        print("ğŸš€ ç°åœ¨å¯ä»¥ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹è¿›è¡Œæ¨ç†äº†")
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­äº†åˆå¹¶è¿‡ç¨‹")
        print("\nâŒ åˆå¹¶è¿‡ç¨‹è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"åˆå¹¶å¤±è´¥: {str(e)}")
        print(f"\nâŒ åˆå¹¶å¤±è´¥: {str(e)}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())